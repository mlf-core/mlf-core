

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Available templates &mdash; mlf-core 1.7.7 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom_cookietemple.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Github Support" href="../github_support.html" />
    <link rel="prev" title="Upgrade mlf-core" href="../upgrade.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> mlf-core
          

          
          </a>

          
            
            
              <div class="version">
                1.7.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../readme.html">mlf-core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">General Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../create.html">Create a project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../list_info.html">Getting information about available templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lint.html">Linting your project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fix_artifact_paths.html">Fixing the paths of locally saved MLflow runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bump_version.html">Bumping the version of an existing project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sync.html">Syncing your project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config.html">Configure mlf-core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../upgrade.html">Upgrade mlf-core</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Available templates</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#mlflow-pytorch">mlflow-pytorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#purpose">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#design">Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#included-frameworks-libraries">Included frameworks/libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#usage">Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#building-the-docker-container">Building the Docker container</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-the-project-with-docker">Running the project with Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-the-project-with-conda">Running the project with Conda</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#faq">FAQ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#i-am-using-docker-but-no-gpus-are-used-for-training">I am using Docker but no GPUs are used for training!</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mlflow-tensorflow">mlflow-tensorflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">Included frameworks/libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id18">Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id19">Building the Docker container</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">Running the project with Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id22">Running the project with Conda</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id24">FAQ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id25">I am using Docker but no GPUs are used for training!</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mlflow-xgboost">mlflow-xgboost</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id27">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id30">Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id31">Included frameworks/libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id38">Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id39">Building the Docker container</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id40">Running the project with Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id42">Running the project with Conda</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id44">FAQ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id45">I am using Docker but no GPUs are used for training!</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mlflow-xgboost-dask">mlflow-xgboost_dask</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id47">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id51">Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id52">Included frameworks/libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id60">Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id61">Building the Docker container</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id62">Running the project with Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id64">Running the project with Conda</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id66">FAQ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id67">I am using Docker but no GPUs are used for training!</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#package-prediction">package-prediction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id69">Purpose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id70">Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id72">Included frameworks/libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#publishing-the-package-to-pypi">Publishing the package to PyPI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#shared-faq">Shared FAQ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#how-do-i-access-my-data-when-running-inside-a-docker-container">How do I access my data when running inside a Docker container?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-do-i-publish-my-documentation">How do I publish my documentation?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id78">Read the Docs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#github-pages">Github Pages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#what-is-dependabot-and-how-do-i-set-it-up">What is Dependabot and how do I set it up?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-do-i-add-a-new-template">How do I add a new template?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../github_support.html">Github Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../creating_releases.html">Creating Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adding_new_templates.html">Adding new templates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_of_conduct.html">Contributor Covenant Code of Conduct</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">mlf-core</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Available templates</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/available_templates/available_templates.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="available-templates">
<span id="id1"></span><h1>Available templates<a class="headerlink" href="#available-templates" title="Permalink to this headline">¶</a></h1>
<p>cookietemple currently has the following templates available:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#mlflow-pytorch">mlflow-pytorch</a></p></li>
<li><p><a class="reference internal" href="#mlflow-tensorflow">mlflow-tensorflow</a></p></li>
<li><p><a class="reference internal" href="#mlflow-xgboost">mlflow-xgboost</a></p></li>
<li><p><a class="reference internal" href="#mlflow-xgboost-dask">mlflow-xgboost_dask</a></p></li>
<li><p><a class="reference internal" href="#package-prediction">package-prediction</a></p></li>
</ol>
<p>In the following every template is devoted its own section, which explains its purpose, design, included frameworks/libraries, usage and frequently asked questions.
A set of frequently questions, which all templates share see here: <a class="reference internal" href="#all-templates-faq"><span class="std std-ref">Shared FAQ</span></a> FAQ.
It is recommended to use the sidebar to navigate this documentation, since it is very long and cumbersome to scroll through.</p>
<div class="section" id="mlflow-pytorch">
<h2>mlflow-pytorch<a class="headerlink" href="#mlflow-pytorch" title="Permalink to this headline">¶</a></h2>
<div class="section" id="purpose">
<h3>Purpose<a class="headerlink" href="#purpose" title="Permalink to this headline">¶</a></h3>
<p>mlflow-pytorch is a <a class="reference external" href="https://mlflow.org/">MLflow</a> based template designed for <a class="reference external" href="https://pytorch.org/">Pytorch</a> machine learning models.
The project is fully CPU and GPU deterministic with <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> integration.
Additionally, Conda and Docker are supported out of the box.</p>
</div>
<div class="section" id="design">
<h3>Design<a class="headerlink" href="#design" title="Permalink to this headline">¶</a></h3>
<p>The package follows the mlf-core convention of a single environment.yml file in conjunction with an mlf-core based Dockerfile.
As required a MLproject file serves as entry point and parameter definition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── AUTHORS.rst
├── .bandit.yml
├── CHANGELOG.rst
├── CODE_OF_CONDUCT.rst
├── Dockerfile
├── docs
│   ├── authors.rst
│   ├── changelog.rst
│   ├── code_of_conduct.rst
│   ├── conf.py
│   ├── index.rst
│   ├── make.bat
│   ├── Makefile
│   ├── model.rst
│   ├── readme.rst
│   ├── requirements.txt
│   ├── _static
│   │   └── custom_cookietemple.css
│   └── usage.rst
├── .editorconfig
├── environment.yml
├── project_slug
│   ├── data_loading
│   │   ├── data_loader.py
│   ├── project_slug.py
│   ├── mlf_core
│   │   ├── mlf_core.py
│   ├── model
│   │   ├── model.py
│   └── training
│       └── train.py
├── .flake8
├── .github
│   ├── ISSUE_TEMPLATE
│   │   ├── bug_report.md
│   │   ├── feature_request.md
│   │   ├── general_question.md
│   │   └── sync_notify.md
│   ├── pull_request_template.md
│   └── workflows
│       ├── build_docs.yml
│       ├── run_mlf_core_lint.yml
│       ├── pr_to_master_from_patch_release_only.yml
│       ├── run_bandit.yml
│       ├── run_flake8_linting.yml
│       ├── sync.yml
│       └── train_cpu.yml
│       └── publish_docker.yml
├── .gitignore
├── LICENSE
├── mlf_core.cfg
├── .mlf_core.yml
├── MLproject
├── README.rst
└── .readthedocs.yml
</pre></div>
</div>
</div>
<div class="section" id="included-frameworks-libraries">
<h3>Included frameworks/libraries<a class="headerlink" href="#included-frameworks-libraries" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://mlflow.org/">MLflow</a> as the primary framework for parameter and artifact logging.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/">Pytorch</a> as the primary machine learning library.</p></li>
<li><p><a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> to fetch all hardware related information.</p></li>
<li><p>Preconfigured <a class="reference external" href="https://readthedocs.org/">readthedocs</a></p></li>
<li><p>Seven Github workflows:</p></li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">build_docs.yml</span></code>, which builds the readthedocs documentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_flake8_linting.yml</span></code>, which runs <a class="reference external" href="https://flake8.pycqa.org/en/latest/">flake8</a> linting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pr_to_master_from_patch_release_only.yml</span></code> Please read <a class="reference internal" href="../github_support.html#pr-master-workflow-docs"><span class="std std-ref">pr_to_master_from_patch_release_only workflow</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_cpu.yml</span></code>, which trains the model on the CPU for a small number of epochs. Requires the data to be accessible.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sync.yml</span></code>, which checks whether a new version of mlflow-pytorch is available and submits a pull request if so.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_mlf_core_lint.yml</span></code>, which runs <code class="docutils literal notranslate"><span class="pre">mlf-core</span> <span class="pre">lint</span></code> to verify that the project adheres to all mlf-core standards.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_bandit.yml</span></code>, which runs <a class="reference external" href="https://pypi.org/project/bandit/">Bandit</a> to find any security issues.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">publish_docker.yml</span></code>, which builds and pushes a Docker container to Github Packages</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h3>
<p>It is strongly advised to use Docker to run mlf-core models, since support for other OS besides Linux is limited and dependency management is greatly simplified.</p>
<div class="section" id="building-the-docker-container">
<h4>Building the Docker container<a class="headerlink" href="#building-the-docker-container" title="Permalink to this headline">¶</a></h4>
<p>The name (=tag) of the Docker Container is specified in the MLproject file in <code class="docutils literal notranslate"><span class="pre">image:</span></code>.
If you created your project with mlf-core’s Github support your Docker container should automatically be building in your Github Container Registry.
Furthermore, it should match the name specified in the MLproject file.
If the Docker container is not available in your Github Container Registry for any reason you must build it locally.
Run: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span> <span class="pre">-t</span> <span class="pre">ghcr.io/GITHUB_USERNAME/PROJECT_SLUG:version</span> <span class="pre">.</span></code>, where <code class="docutils literal notranslate"><span class="pre">PROJECT_SLUG</span></code> is your project’s name and <code class="docutils literal notranslate"><span class="pre">version</span></code> the current project version.
The MLproject file can always be examined to determine the required Docker container tag.</p>
</div>
<div class="section" id="running-the-project-with-docker">
<h4>Running the project with Docker<a class="headerlink" href="#running-the-project-with-docker" title="Permalink to this headline">¶</a></h4>
<p>After having build the Docker container you can now launch your project with <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
The Docker container will automatically spin up.</p>
<p><strong>Note</strong> if you want to run your project with GPU support you must have the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed.
Moreover, you need to pass additional Docker runtime arguments e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span> <span class="pre">-A</span> <span class="pre">gpus=all</span></code>, which makes all available GPUs accessible to the Docker container.</p>
</div>
<div class="section" id="running-the-project-with-conda">
<h4>Running the project with Conda<a class="headerlink" href="#running-the-project-with-conda" title="Permalink to this headline">¶</a></h4>
<p>Running the project using Conda is possible, but discouraged, since <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> currently only really supports Linux.
Comment out <code class="docutils literal notranslate"><span class="pre">docker_env</span></code> and comment in <code class="docutils literal notranslate"><span class="pre">conda_env</span></code>. Now run the project using e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
GPUs will be automatically be detected and used.</p>
</div>
</div>
<div class="section" id="faq">
<h3>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline">¶</a></h3>
<div class="section" id="i-am-using-docker-but-no-gpus-are-used-for-training">
<h4>I am using Docker but no GPUs are used for training!<a class="headerlink" href="#i-am-using-docker-but-no-gpus-are-used-for-training" title="Permalink to this headline">¶</a></h4>
<p>Please ensure that you have CUDA configured, the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed and pass <code class="docutils literal notranslate"><span class="pre">-A</span> <span class="pre">gpus=all</span></code> when running the project.</p>
</div>
</div>
</div>
<div class="section" id="mlflow-tensorflow">
<h2>mlflow-tensorflow<a class="headerlink" href="#mlflow-tensorflow" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id7">
<h3>Purpose<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>mlflow-tensorflow is a <a class="reference external" href="https://mlflow.org/">MLflow</a> based template designed for <a class="reference external" href="https://www.tensorflow.org/">Tensorflow (version 2+)</a> machine learning models.
The project is fully CPU and GPU deterministic with <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> integration.
Additionally, Conda and Docker are supported out of the box.</p>
</div>
<div class="section" id="id10">
<h3>Design<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>The package follows the mlf-core convention of a single environment.yml file in conjunction with an mlf-core based Dockerfile.
As required a MLproject file serves as entry point and parameter definition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── AUTHORS.rst
├── .bandit.yml
├── CHANGELOG.rst
├── CODE_OF_CONDUCT.rst
├── Dockerfile
├── docs
│   ├── authors.rst
│   ├── changelog.rst
│   ├── code_of_conduct.rst
│   ├── conf.py
│   ├── index.rst
│   ├── make.bat
│   ├── Makefile
│   ├── model.rst
│   ├── readme.rst
│   ├── requirements.txt
│   ├── _static
│   │   └── custom_cookietemple.css
│   └── usage.rst
├── .editorconfig
├── environment.yml
├── exploding_springfield
│   ├── data_loading
│   │   ├── data_loader.py
│   ├── exploding_springfield.py
│   ├── mlf_core
│   │   ├── mlf_core.py
│   ├── model
│   │   ├── model.py
│   └── training
│       └── train.py
├── .flake8
├── .github
│   ├── ISSUE_TEMPLATE
│   │   ├── bug_report.md
│   │   ├── feature_request.md
│   │   ├── general_question.md
│   │   └── sync_notify.md
│   ├── pull_request_template.md
│   └── workflows
│       ├── build_docs.yml
│       ├── mlf_core_lint.yml
│       ├── pr_to_master_from_patch_release_only.yml
│       ├── run_bandit.yml
│       ├── run_flake8_linting.yml
│       ├── sync.yml
│       └── train_cpu.yml
│       └── publish_docker.yml
├── .gitignore
├── LICENSE
├── mlf_core.cfg
├── .mlf_core.yml
├── MLproject
├── README.rst
└── .readthedocs.yml
</pre></div>
</div>
</div>
<div class="section" id="id11">
<h3>Included frameworks/libraries<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://mlflow.org/">MLflow</a> as the primary framework for parameter and artifact logging.</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/">Tensorflow (version 2+)</a> as the primary machine learning library.</p></li>
<li><p><a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> to fetch all hardware related information.</p></li>
<li><p>Preconfigured <a class="reference external" href="https://readthedocs.org/">readthedocs</a></p></li>
<li><p>Five Github workflows:</p></li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">build_docs.yml</span></code>, which builds the readthedocs documentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_flake8_linting.yml</span></code>, which runs <a class="reference external" href="https://flake8.pycqa.org/en/latest/">flake8</a> linting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pr_to_master_from_patch_release_only.yml</span></code> Please read <a class="reference internal" href="../github_support.html#pr-master-workflow-docs"><span class="std std-ref">pr_to_master_from_patch_release_only workflow</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_cpu.yml</span></code>, which trains the model on the CPU for a small number of epochs. Requires the data to be accessible.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sync.yml</span></code>, which checks whether a new version of mlflow-pytorch is available and submits a pull request if so.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_mlf_core_lint.yml</span></code>, which runs <code class="docutils literal notranslate"><span class="pre">mlf-core</span> <span class="pre">lint</span></code> to verify that the project adheres to all mlf-core standards.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_bandit.yml</span></code>, which runs <a class="reference external" href="https://pypi.org/project/bandit/">Bandit</a> to find any security issues.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">publish_docker.yml</span></code>, which builds and pushes a Docker container to Github Packages</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="id18">
<h3>Usage<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>It is strongly advised to use Docker to run mlf-core models, since support for other OS besides Linux is limited and dependency management is greatly simplified.</p>
<div class="section" id="id19">
<h4>Building the Docker container<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h4>
<p>The name (=tag) of the Docker Container is specified in the MLproject file in <code class="docutils literal notranslate"><span class="pre">image:</span></code>.
If you created your project with mlf-core’s Github support your Docker container should automatically be building in your Github Container Registry.
Furthermore, it should match the name specified in the MLproject file.
If the Docker container is not available in your Github Container Registry for any reason you must build it locally.
Run: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span> <span class="pre">-t</span> <span class="pre">ghcr.io/GITHUB_USERNAME/PROJECT_SLUG:version</span> <span class="pre">.</span></code>, where <code class="docutils literal notranslate"><span class="pre">PROJECT_SLUG</span></code> is your project’s name and <code class="docutils literal notranslate"><span class="pre">version</span></code> the current project version.
The MLproject file can always be examined to determine the required Docker container tag.</p>
</div>
<div class="section" id="id20">
<h4>Running the project with Docker<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h4>
<p>After having build the Docker container you can now launch your project with <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
The Docker container will automatically spin up.</p>
<p><strong>Note</strong> if you want to run your project with GPU support you must have the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed.
Moreover, you need to pass additional Docker runtime arguments e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span> <span class="pre">-A</span> <span class="pre">gpus=all</span></code>, which makes all available GPUs accessible to the Docker container.</p>
</div>
<div class="section" id="id22">
<h4>Running the project with Conda<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h4>
<p>Running the project using Conda is possible, but discouraged, since <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> currently only really supports Linux.
Comment out <code class="docutils literal notranslate"><span class="pre">docker_env</span></code> and comment in <code class="docutils literal notranslate"><span class="pre">conda_env</span></code>. Now run the project using e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
GPUs will be automatically be detected and used.</p>
</div>
</div>
<div class="section" id="id24">
<h3>FAQ<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id25">
<h4>I am using Docker but no GPUs are used for training!<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h4>
<p>Please ensure that you have CUDA configured, the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed and pass <code class="docutils literal notranslate"><span class="pre">-A</span> <span class="pre">gpus=all</span></code> when running the project.</p>
</div>
</div>
</div>
<div class="section" id="mlflow-xgboost">
<h2>mlflow-xgboost<a class="headerlink" href="#mlflow-xgboost" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id27">
<h3>Purpose<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<p>mlflow-xgboost is a <a class="reference external" href="https://mlflow.org/">MLflow</a> based template designed for <a class="reference external" href="https://xgboost.readthedocs.io">XGBoost</a> machine learning models.
The project is fully CPU and GPU deterministic with <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> integration.
Additionally, Conda and Docker are supported out of the box.</p>
</div>
<div class="section" id="id30">
<h3>Design<a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h3>
<p>The package follows the mlf-core convention of a single environment.yml file in conjunction with an mlf-core based Dockerfile.
As required a MLproject file serves as entry point and parameter definition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── AUTHORS.rst
├── .bandit.yml
├── CHANGELOG.rst
├── CODE_OF_CONDUCT.rst
├── Dockerfile
├── docs
│   ├── authors.rst
│   ├── changelog.rst
│   ├── code_of_conduct.rst
│   ├── conf.py
│   ├── index.rst
│   ├── make.bat
│   ├── Makefile
│   ├── model.rst
│   ├── readme.rst
│   ├── requirements.txt
│   ├── _static
│   │   └── custom_cookietemple.css
│   └── usage.rst
├── .editorconfig
├── environment.yml
├── exploding_springfield
│   ├── data_loading
│   │   ├── data_loader.py
│   ├── exploding_springfield.py
│   ├── mlf_core
│   │   ├── mlf_core.py
├── .flake8
├── .github
│   ├── ISSUE_TEMPLATE
│   │   ├── bug_report.md
│   │   ├── feature_request.md
│   │   ├── general_question.md
│   │   └── sync_notify.md
│   ├── pull_request_template.md
│   └── workflows
│       ├── build_docs.yml
│       ├── mlf_core_lint.yml
│       ├── pr_to_master_from_patch_release_only.yml
│       ├── run_bandit.yml
│       ├── run_flake8_linting.yml
│       ├── sync.yml
│       └── train_cpu.yml
│       └── publish_docker.yml
├── .gitignore
├── LICENSE
├── mlf_core.cfg
├── .mlf_core.yml
├── MLproject
├── README.rst
└── .readthedocs.yml
</pre></div>
</div>
</div>
<div class="section" id="id31">
<h3>Included frameworks/libraries<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://mlflow.org/">MLflow</a> as the primary framework for parameter and artifact logging.</p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io">XGBoost</a> as the primary machine learning library.</p></li>
<li><p><a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> to fetch all hardware related information.</p></li>
<li><p>Preconfigured <a class="reference external" href="https://readthedocs.org/">readthedocs</a></p></li>
<li><p>Five Github workflows:</p></li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">build_docs.yml</span></code>, which builds the readthedocs documentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_flake8_linting.yml</span></code>, which runs <a class="reference external" href="https://flake8.pycqa.org/en/latest/">flake8</a> linting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pr_to_master_from_patch_release_only.yml</span></code> Please read <a class="reference internal" href="../github_support.html#pr-master-workflow-docs"><span class="std std-ref">pr_to_master_from_patch_release_only workflow</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_cpu.yml</span></code>, which trains the model on the CPU for a small number of epochs. Requires the data to be accessible.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sync.yml</span></code>, which checks whether a new version of mlflow-pytorch is available and submits a pull request if so.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_mlf_core_lint.yml</span></code>, which runs <code class="docutils literal notranslate"><span class="pre">mlf-core</span> <span class="pre">lint</span></code> to verify that the project adheres to all mlf-core standards.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_bandit.yml</span></code>, which runs <a class="reference external" href="https://pypi.org/project/bandit/">Bandit</a> to find any security issues.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">publish_docker.yml</span></code>, which builds and pushes a Docker container to Github Packages</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="id38">
<h3>Usage<a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h3>
<p>It is strongly advised to use Docker to run mlf-core models, since support for other OS besides Linux is limited and dependency management is greatly simplified.</p>
<div class="section" id="id39">
<h4>Building the Docker container<a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h4>
<p>The name (=tag) of the Docker Container is specified in the MLproject file in <code class="docutils literal notranslate"><span class="pre">image:</span></code>.
If you created your project with mlf-core’s Github support your Docker container should automatically be building in your Github Container Registry.
Furthermore, it should match the name specified in the MLproject file.
If the Docker container is not available in your Github Container Registry for any reason you must build it locally.
Run: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span> <span class="pre">-t</span> <span class="pre">ghcr.io/GITHUB_USERNAME/PROJECT_SLUG:version</span> <span class="pre">.</span></code>, where <code class="docutils literal notranslate"><span class="pre">PROJECT_SLUG</span></code> is your project’s name and <code class="docutils literal notranslate"><span class="pre">version</span></code> the current project version.
The MLproject file can always be examined to determine the required Docker container tag.</p>
</div>
<div class="section" id="id40">
<h4>Running the project with Docker<a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h4>
<p>After having build the Docker container you can now launch your project with <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
The Docker container will automatically spin up.</p>
<p><strong>Note</strong> if you want to run your project with GPU support you must have the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed.
Moreover, you need to pass additional Docker runtime arguments e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span> <span class="pre">-A</span> <span class="pre">gpus=all</span></code>, which makes all available GPUs accessible to the Docker container.</p>
</div>
<div class="section" id="id42">
<h4>Running the project with Conda<a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h4>
<p>Running the project using Conda is possible, but discouraged, since <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> currently only really supports Linux.
Comment out <code class="docutils literal notranslate"><span class="pre">docker_env</span></code> and comment in <code class="docutils literal notranslate"><span class="pre">conda_env</span></code>. Now run the project using e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
GPUs will be automatically be detected and used.</p>
</div>
</div>
<div class="section" id="id44">
<h3>FAQ<a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id45">
<h4>I am using Docker but no GPUs are used for training!<a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h4>
<p>Please ensure that you have CUDA configured, the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed and pass <code class="docutils literal notranslate"><span class="pre">-A</span> <span class="pre">gpus=all</span></code> when running the project.</p>
</div>
</div>
</div>
<div class="section" id="mlflow-xgboost-dask">
<h2>mlflow-xgboost_dask<a class="headerlink" href="#mlflow-xgboost-dask" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id47">
<h3>Purpose<a class="headerlink" href="#id47" title="Permalink to this headline">¶</a></h3>
<p>mlflow-xgboost_dask is a <a class="reference external" href="https://mlflow.org/">MLflow</a> based template designed for <a class="reference external" href="https://xgboost.readthedocs.io">XGBoost</a> machine learning models.
Furthermore, multi GPU supported is facilitated <em>via</em> a local <a class="reference external" href="https://dask.org/">Dask</a> cluster. Be aware that multi GPU reproducibility is work in progress.
The project is fully CPU and GPU deterministic with <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> integration.
Additionally, Conda and Docker are supported out of the box.</p>
</div>
<div class="section" id="id51">
<h3>Design<a class="headerlink" href="#id51" title="Permalink to this headline">¶</a></h3>
<p>The package follows the mlf-core convention of a single environment.yml file in conjunction with an mlf-core based Dockerfile.
As required a MLproject file serves as entry point and parameter definition.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── AUTHORS.rst
├── .bandit.yml
├── CHANGELOG.rst
├── CODE_OF_CONDUCT.rst
├── Dockerfile
├── docs
│   ├── authors.rst
│   ├── changelog.rst
│   ├── code_of_conduct.rst
│   ├── conf.py
│   ├── index.rst
│   ├── make.bat
│   ├── Makefile
│   ├── model.rst
│   ├── readme.rst
│   ├── requirements.txt
│   ├── _static
│   │   └── custom_cookietemple.css
│   └── usage.rst
├── .editorconfig
├── environment.yml
├── exploding_springfield
│   ├── data_loading
│   │   ├── data_loader.py
│   ├── exploding_springfield.py
│   ├── mlf_core
│   │   ├── mlf_core.py
├── .flake8
├── .github
│   ├── ISSUE_TEMPLATE
│   │   ├── bug_report.md
│   │   ├── feature_request.md
│   │   ├── general_question.md
│   │   └── sync_notify.md
│   ├── pull_request_template.md
│   └── workflows
│       ├── build_docs.yml
│       ├── mlf_core_lint.yml
│       ├── pr_to_master_from_patch_release_only.yml
│       ├── run_bandit.yml
│       ├── run_flake8_linting.yml
│       ├── sync.yml
│       └── train_cpu.yml
│       └── publish_docker.yml
├── .gitignore
├── LICENSE
├── mlf_core.cfg
├── .mlf_core.yml
├── MLproject
├── README.rst
└── .readthedocs.yml
</pre></div>
</div>
</div>
<div class="section" id="id52">
<h3>Included frameworks/libraries<a class="headerlink" href="#id52" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://mlflow.org/">MLflow</a> as the primary framework for parameter and artifact logging.</p></li>
<li><p><a class="reference external" href="https://xgboost.readthedocs.io">XGBoost</a> as the primary machine learning library.</p></li>
<li><p><a class="reference external" href="https://dask.org/">Dask</a> to create a local (Cuda) cluster and facilitate multi GPU support.</p></li>
<li><p><a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> to fetch all hardware related information.</p></li>
<li><p>Preconfigured <a class="reference external" href="https://readthedocs.org/">readthedocs</a></p></li>
<li><p>Five Github workflows:</p></li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">build_docs.yml</span></code>, which builds the readthedocs documentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_flake8_linting.yml</span></code>, which runs <a class="reference external" href="https://flake8.pycqa.org/en/latest/">flake8</a> linting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pr_to_master_from_patch_release_only.yml</span></code> Please read <a class="reference internal" href="../github_support.html#pr-master-workflow-docs"><span class="std std-ref">pr_to_master_from_patch_release_only workflow</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_cpu.yml</span></code>, which trains the model on the CPU for a small number of epochs. Requires the data to be accessible.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sync.yml</span></code>, which checks whether a new version of mlflow-pytorch is available and submits a pull request if so.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_mlf_core_lint.yml</span></code>, which runs <code class="docutils literal notranslate"><span class="pre">mlf-core</span> <span class="pre">lint</span></code> to verify that the project adheres to all mlf-core standards.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_bandit.yml</span></code>, which runs <a class="reference external" href="https://pypi.org/project/bandit/">Bandit</a> to find any security issues.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">publish_docker.yml</span></code>, which builds and pushes a Docker container to Github Packages</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="id60">
<h3>Usage<a class="headerlink" href="#id60" title="Permalink to this headline">¶</a></h3>
<p>It is strongly advised to use Docker to run mlf-core models, since support for other OS besides Linux is limited and dependency management is greatly simplified.</p>
<div class="section" id="id61">
<h4>Building the Docker container<a class="headerlink" href="#id61" title="Permalink to this headline">¶</a></h4>
<p>The name (=tag) of the Docker Container is specified in the MLproject file in <code class="docutils literal notranslate"><span class="pre">image:</span></code>.
If you created your project with mlf-core’s Github support your Docker container should automatically be building in your Github Container Registry.
Furthermore, it should match the name specified in the MLproject file.
If the Docker container is not available in your Github Container Registry for any reason you must build it locally.
Run: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span> <span class="pre">-t</span> <span class="pre">ghcr.io/GITHUB_USERNAME/PROJECT_SLUG:version</span> <span class="pre">.</span></code>, where <code class="docutils literal notranslate"><span class="pre">PROJECT_SLUG</span></code> is your project’s name and <code class="docutils literal notranslate"><span class="pre">version</span></code> the current project version.
The MLproject file can always be examined to determine the required Docker container tag.</p>
</div>
<div class="section" id="id62">
<h4>Running the project with Docker<a class="headerlink" href="#id62" title="Permalink to this headline">¶</a></h4>
<p>After having build the Docker container you can now launch your project with <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
The Docker container will automatically spin up.</p>
<p><strong>Note</strong> if you want to run your project with GPU support you must have the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed.
Moreover, you need to pass additional Docker runtime arguments e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span> <span class="pre">-A</span> <span class="pre">gpus=all</span></code>, which makes all available GPUs accessible to the Docker container.</p>
</div>
<div class="section" id="id64">
<h4>Running the project with Conda<a class="headerlink" href="#id64" title="Permalink to this headline">¶</a></h4>
<p>Running the project using Conda is possible, but discouraged, since <a class="reference external" href="https://github.com/mlf-core/system-intelligence">system-intelligence</a> currently only really supports Linux.
Comment out <code class="docutils literal notranslate"><span class="pre">docker_env</span></code> and comment in <code class="docutils literal notranslate"><span class="pre">conda_env</span></code>. Now run the project using e.g. <code class="docutils literal notranslate"><span class="pre">mlflow</span> <span class="pre">run</span> <span class="pre">.</span></code>.
GPUs will be automatically be detected and used.</p>
</div>
</div>
<div class="section" id="id66">
<h3>FAQ<a class="headerlink" href="#id66" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id67">
<h4>I am using Docker but no GPUs are used for training!<a class="headerlink" href="#id67" title="Permalink to this headline">¶</a></h4>
<p>Please ensure that you have CUDA configured, the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker">NVIDIA Container Toolkit</a> installed and pass <code class="docutils literal notranslate"><span class="pre">-A</span> <span class="pre">gpus=all</span></code> when running the project.</p>
</div>
</div>
</div>
<div class="section" id="package-prediction">
<h2>package-prediction<a class="headerlink" href="#package-prediction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id69">
<h3>Purpose<a class="headerlink" href="#id69" title="Permalink to this headline">¶</a></h3>
<p>package-prediction is a template designed to easily distribute <a class="reference external" href="https://pypi.org/">PyPI</a> packages of machine learning models.
The template only provides the boilerplate code to load models and perform predictions. Data wrangling or model training should be done using the mlflow templates.</p>
</div>
<div class="section" id="id70">
<h3>Design<a class="headerlink" href="#id70" title="Permalink to this headline">¶</a></h3>
<p>The package is closely related to <a class="reference external" href="cookietemple'shttps://github.com/cookiejar/cookietemple">cookietemple’shttps://github.com/cookiejar/cookietemple</a> <a class="reference external" href="https://cookietemple.readthedocs.io/en/latest/available_templates/available_templates.html#cli-python">cli-python</a> template.
It is primarily based on setuptools to build the package and uses a Github workflow to easily upload the package to <a class="reference external" href="https://pypi.org/">PyPI</a>.
Any prediction boiler code is machine learning library specific.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── AUTHORS.rst
├── .bandit.yml
├── CHANGELOG.rst
├── CODE_OF_CONDUCT.rst
├── .coveragerc
├── Dockerfile
├── docs
│   ├── authors.rst
│   ├── changelog.rst
│   ├── code_of_conduct.rst
│   ├── conf.py
│   ├── index.rst
│   ├── installation.rst
│   ├── make.bat
│   ├── Makefile
│   ├── readme.rst
│   ├── requirements.txt
│   ├── _static
│   │   └── custom_cookietemple.css
│   └── usage.rst
├── .editorconfig
├── exploding_springfield
│   ├── cli_pytorch.py
│   ├── cli_tensorflow.py
│   ├── cli_xgboost.py
│   ├── data
│   │   └── xgboost_test_data.tsv
│   ├── __init__.py
│   ├── models
│   │   └── xgboost_test_model.xgb
├── .gitattributes
├── .github
│   ├── dependabot.yml
│   ├── ISSUE_TEMPLATE
│   │   ├── bug_report.md
│   │   ├── feature_request.md
│   │   └── general_question.md
│   ├── pull_request_template.md
│   └── workflows
│       ├── build_docs.yml
│       ├── build_package.yml
│       ├── pr_to_master_from_patch_release_only.yml
│       ├── publish_package.yml
│       ├── run_bandit.yml
│       ├── run_flake8_linting.yml
│       ├── run_mlf_core_lint.yml
│       └── sync.yml
├── .gitignore
├── LICENSE
├── Makefile
├── makefiles
│   ├── Linux.mk
│   └── Windows.mk
├── MANIFEST.in
├── mlf_core.cfg
├── .mlf_core.yml
├── README.rst
├── .readthedocs.yml
├── requirements_dev.txt
├── requirements.txt
├── setup.cfg
└── setup.py
</pre></div>
</div>
</div>
<div class="section" id="id72">
<h3>Included frameworks/libraries<a class="headerlink" href="#id72" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://setuptools.readthedocs.io/en/latest/">setuptools</a> for code packaging</p></li>
<li><p><a class="reference external" href="https://click.palletsprojects.com/">click</a> for the command line interface</p></li>
<li><p>One of <a class="reference external" href="https://pytorch.org/">Pytorch</a>, <a class="reference external" href="https://www.tensorflow.org/">Tensorflow</a> or <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a>,</p></li>
<li><p>Preconfigured <a class="reference external" href="https://readthedocs.org/">readthedocs</a></p></li>
<li><p>Six Github workflows:</p></li>
</ol>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">build_docs.yml</span></code>, which builds the readthedocs documentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">build_package.yml</span></code>, which builds the cli-python package.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_flake8_linting.yml</span></code>, which runs <a class="reference external" href="https://flake8.pycqa.org/en/latest/">flake8</a> linting.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">publish_package.yml</span></code>, which publishes the package to PyPi. Note that it only runs on Github release and requires PyPi secrets to be set up.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_bandit</span></code>, run <a class="reference external" href="https://github.com/PyCQA/bandit">bandit</a> to discover security issues in your python code</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pr_to_master_from_patch_release_only</span></code>: Please read <a class="reference internal" href="../github_support.html#pr-master-workflow-docs"><span class="std std-ref">pr_to_master_from_patch_release_only workflow</span></a>.</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="publishing-the-package-to-pypi">
<h3>Publishing the package to PyPI<a class="headerlink" href="#publishing-the-package-to-pypi" title="Permalink to this headline">¶</a></h3>
<p>Ensure that your package builds and passes any twine checks. The <code class="docutils literal notranslate"><span class="pre">build_package.yml</span></code> workflow verifies both.
If the workflow passes you should open a pull request to <code class="docutils literal notranslate"><span class="pre">master</span></code> and merge it after reviews.
The only thing left to do now is to create a release on Github.
<strong>Ensure that your PyPI secrets are set.</strong> Follow the instructions on <a class="reference external" href="https://docs.github.com/en/free-pro-team&#64;latest/actions/reference/encrypted-secrets">Encrypted Secrets</a> if required.</p>
</div>
</div>
<div class="section" id="shared-faq">
<span id="all-templates-faq"></span><h2>Shared FAQ<a class="headerlink" href="#shared-faq" title="Permalink to this headline">¶</a></h2>
<div class="section" id="how-do-i-access-my-data-when-running-inside-a-docker-container">
<h3>How do I access my data when running inside a Docker container?<a class="headerlink" href="#how-do-i-access-my-data-when-running-inside-a-docker-container" title="Permalink to this headline">¶</a></h3>
<p>mlf-core projects by default mount <code class="docutils literal notranslate"><span class="pre">/data</span></code> to <code class="docutils literal notranslate"><span class="pre">/data</span></code> inside the Docker container. Hence, add a <code class="docutils literal notranslate"><span class="pre">/data</span></code> folder and access the files by assuming that the files are in <code class="docutils literal notranslate"><span class="pre">/data</span></code>.</p>
</div>
<div class="section" id="how-do-i-publish-my-documentation">
<h3>How do I publish my documentation?<a class="headerlink" href="#how-do-i-publish-my-documentation" title="Permalink to this headline">¶</a></h3>
<p>mlf-core ships with a full, production ready <a class="reference external" href="https://readthedocs.org/">Read the Docs</a> setup and with a complete gh-pages setup.</p>
<div class="section" id="id78">
<h4>Read the Docs<a class="headerlink" href="#id78" title="Permalink to this headline">¶</a></h4>
<p>You need to <a class="reference external" href="https://docs.readthedocs.io/en/stable/intro/import-guide.html">import your documentation</a> on Read the Docs website.
Do not forget to sync your account first to see your repository.
Your documentation will then be available on <code class="docutils literal notranslate"><span class="pre">https://repositoryname.readthedocs.io/</span></code></p>
</div>
<div class="section" id="github-pages">
<h4>Github Pages<a class="headerlink" href="#github-pages" title="Permalink to this headline">¶</a></h4>
<p>Your documentation is automatically pushed to the <code class="docutils literal notranslate"><span class="pre">gh-pages</span></code> branch. Follow the documentation on
<code class="docutils literal notranslate"><span class="pre">configuring</span> <span class="pre">a</span> <span class="pre">publishing</span> <span class="pre">source</span> <span class="pre">for</span> <span class="pre">your</span> <span class="pre">Github</span> <span class="pre">pages</span> <span class="pre">site</span> <span class="pre">&lt;https://docs.github.com/en/free-pro-team&#64;latest/github/working-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site&gt;`_</span>
<span class="pre">and</span> <span class="pre">select</span> <span class="pre">the</span> <span class="pre">gh-pages</span> <span class="pre">branch.</span> <span class="pre">Your</span> <span class="pre">documentation</span> <span class="pre">will</span> <span class="pre">then</span> <span class="pre">be</span> <span class="pre">available</span> <span class="pre">on</span> <span class="pre">``https://username.github.io/repositoryname</span></code>.</p>
</div>
</div>
<div class="section" id="what-is-dependabot-and-how-do-i-set-it-up">
<h3>What is Dependabot and how do I set it up?<a class="headerlink" href="#what-is-dependabot-and-how-do-i-set-it-up" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://dependabot.com/">Dependabot</a> is a service, which (for supported languages) automatically submits pull requests for dependency updates.
cookietemple templates ship with dependabot configurations, if the language is supported by Dependabot.
To enable Dependabot you need to login (with your Github account) and add your repository (or enable Dependabot for all repositories).
Note that you need to do this for every organization separately. Dependabot will then pick up the configuration and start submitting pull requests!</p>
</div>
<div class="section" id="how-do-i-add-a-new-template">
<h3>How do I add a new template?<a class="headerlink" href="#how-do-i-add-a-new-template" title="Permalink to this headline">¶</a></h3>
<p>Please follow <a class="reference internal" href="../adding_new_templates.html#adding-templates"><span class="std std-ref">Adding new templates</span></a>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../github_support.html" class="btn btn-neutral float-right" title="Github Support" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../upgrade.html" class="btn btn-neutral float-left" title="Upgrade mlf-core" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Lukas Heumos

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>